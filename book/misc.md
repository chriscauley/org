---
layout: post
title: "Turing Sympathy Test"
description: "A test to determine whether or not a person has a prejudice against machines"
category: 
tags: []
---
{% include JB/setup %}

I had an imaginary conversation in my head today:

Bob: Computers could never have free will.

Anne: I guess to discuss this we need to decide what free will is. When a human analyzes information and makes a choice, what makes it special in a way that a computer could not.

Bob: Well that's easy. A computer just does what it is programmed to do.

Anne: I think I have an example that we could use to better justify free will. Let's say you go to a movie rental store - 

Bob: Well computers definitely don't have free will now since it's 1998.

Anne: Bear with me. You go to rent a movie and ask the guy behind the counter for a recommendation. He asks the last five movies you liked and the last five movies you hated. He then tells you a new movie. Did he make a choice? Did he exercise free will?

Bob: I guess so.

Anne: Well what if he then told you a microphone was listening on your conversation and he got the recommendation from the computer in front of him.

Bob: That wouldn't be free will since the computer is just looking up movies of similar types and giving a recommendation.

Anne: Well, actually the microphone goes to a datacenter with the worlds best movie critics, one of whom was listening in at the time.

Bob: Okay...

Anne: But then he gets an email saying that the critic based recommendation system was replaced by an AI earlier that day. But actually he read the timezone on the email wrong and the human to computer switch won't happen for another hour.

Bob: Alright I get it. 

Anne: Then he laughs and says that he made that all up to fuck with you. Is his recommendation based in free will. 

Bob: What's your point? Get to the point already.

Anne: and then the human claims that he's an android, before he shoots himself in the head. You see blood but when you look closer you see it's actually just blood colored oil. Is his death any less tragic because you discover he's not actually a human.

Bob: What does this have to do with defining free will? We're getting off track.

Anne: Well, we aren't any closer to telling whether or not computers could have free will, but have shown that you are prejudiced against a computer and will insist that any desicision made by a human is free will and any made by a computer is not.

Bob: That's because the human isn't just following a preprogrammed algorithm. Really the programmer who wrote the program made the choice, indirectly.

Anne: Even if it's a genetic algorithm that the computer is using? One that grows through randomness and adaptation like our DNA?

Bob: Yes, it's still just an algorithm.

Anne: So if the clerk was a human who looked up the recommendation in a chart and then chose one of the best recommendations randomly, then the human doesn't have free will?

Bob: Yes he does, he's just CHOOSING not to use it, which is also using free will.

Anne: So I guess the difference is that free will is the choice to lie and act maliciously.

Bob: I guess so, can we talk about something else?

Anne: Sure.

Anne then switched off the program named Bob and started filling out a bug report, attaching the above conversation.
